# -*- coding: utf-8 -*-
"""Pearls_AQI_Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mrZWr4RRxyfIx6tFO5Z0xgT77MuwN5gc

üß© Overview ‚Äî What this Colab script will do

Fetch hourly data from Open-Meteo
 for:

Air quality (pm2_5, pm10, no2, o3, so2, co)

Weather (temperature_2m, relative_humidity_2m, surface_pressure, wind_speed_10m, wind_direction_10m)

Loop month-by-month to safely fetch 1 year (avoid timeout or large responses).

Merge both datasets on timestamps.

Clean & standardize columns.

Save to CSV/Parquet (for now, in Colab).

next step (feature creation ‚Üí Hopsworks insert).
"""

!pip install pandas requests tqdm pyarrow --quiet

import pandas as pd
import requests
from datetime import date, timedelta
from dateutil.relativedelta import relativedelta
from tqdm import tqdm

# 2Ô∏è Configurations
latitude = 24.8607    # khi
longitude = 67.0011
start_date = date(2024, 1, 1)
end_date = date(2025, 1, 1)

# base urls
URL_WEATHER = "https://archive-api.open-meteo.com/v1/archive"
URL_AIR = "https://air-quality-api.open-meteo.com/v1/air-quality"

# 3Ô∏è Helper function to fetch a monthly chunk
def fetch_open_meteo_chunk(lat, lon, start_dt, end_dt):
    """Fetch weather + air-quality data for a date range (1 month typical)"""

    params_weather = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_dt.isoformat(),
        "end_date": end_dt.isoformat(),
        "hourly": ["temperature_2m", "relative_humidity_2m", "surface_pressure",
                   "wind_speed_10m", "wind_direction_10m"],
        "timezone": "auto"
    }

    params_air = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_dt.isoformat(),
        "end_date": end_dt.isoformat(),
        "hourly": ["pm10", "pm2_5", "carbon_monoxide", "nitrogen_dioxide",
                   "sulphur_dioxide", "ozone"],
        "timezone": "auto"
    }

    # fetch both
    w = requests.get(URL_WEATHER, params=params_weather, timeout=120).json()
    a = requests.get(URL_AIR, params=params_air, timeout=120).json()

    # convert to DataFrames
    df_weather = pd.DataFrame(w["hourly"])
    df_air = pd.DataFrame(a["hourly"])

    # merge on time
    df = pd.merge(df_weather, df_air, on="time", how="outer")
    df["time"] = pd.to_datetime(df["time"])
    return df.sort_values("time")

# 4Ô∏è Loop for 12 months (1 year backfill)
import time
from requests.exceptions import Timeout, RequestException

frames = []
current = start_date
max_retries = 3

while current < end_date:
    chunk_end = min(current + relativedelta(months=1) - timedelta(days=1), end_date)
    print(f"Fetching {current} ‚Üí {chunk_end}")

    # Retry logic for each chunk
    for attempt in range(max_retries):
        try:
            df_chunk = fetch_open_meteo_chunk(latitude, longitude, current, chunk_end)
            frames.append(df_chunk)
            print(f" Success!")
            time.sleep(1)  # Small delay to avoid rate limiting
            break  # Success! Exit retry loop

        except (Timeout, RequestException) as e:
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)  # Wait 5, 10, 15 seconds
                print(f" Attempt {attempt + 1} failed. Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"Failed after {max_retries} attempts!")
                raise  # Give up after 3 tries

    current += relativedelta(months=1)

# combine all months
print(f"\n Combining {len(frames)} months of data...")
df_all = pd.concat(frames, ignore_index=True)
df_all = df_all.drop_duplicates(subset=["time"]).sort_values("time")
print(f" Total records: {len(df_all)}")

# 5Ô∏è Basic cleaning / renaming
df_all.rename(columns={
    "time": "timestamp",
    "temperature_2m": "temp_C",
    "relative_humidity_2m": "humidity_percent",
    "surface_pressure": "pressure_hPa",
    "wind_speed_10m": "wind_speed_mps",
    "wind_direction_10m": "wind_deg",
    "pm2_5": "pm2_5_ugm3",
    "pm10": "pm10_ugm3",
    "carbon_monoxide": "co_ugm3",
    "nitrogen_dioxide": "no2_ugm3",
    "sulphur_dioxide": "so2_ugm3",
    "ozone": "o3_ugm3"
}, inplace=True)

# add city & coordinates
df_all["city"] = "karachi"
df_all["latitude"] = latitude
df_all["longitude"] = longitude

# reorder columns
cols = ["timestamp", "city", "latitude", "longitude",
        "temp_C", "humidity_percent", "pressure_hPa",
        "wind_speed_mps", "wind_deg",
        "pm2_5_ugm3", "pm10_ugm3", "co_ugm3", "no2_ugm3", "so2_ugm3", "o3_ugm3"]
df_all = df_all[cols]

# 6Ô∏è Save locally in Colab
df_all.to_csv("karachi_air_weather_2024.csv", index=False)
df_all.to_parquet("karachi_air_weather_2024.parquet", index=False)

print("‚úÖ Done! Rows:", len(df_all))
df_all.head()

from google.colab import drive
drive.mount('/content/drive')

output_path = "/content/drive/MyDrive/air_quality_data/"
import os
os.makedirs(output_path, exist_ok=True)
df_all.to_csv(output_path + "karachi_air_weather_2024.csv", index=False)
df_all.to_parquet(output_path + "karachi_air_weather_2024.parquet", index=False)

"""Compute Features from Raw Data"""

print(df_all.columns.tolist())

import os
import numpy as np

# load the raw merged file you created earlier
input_path = "/content/drive/MyDrive/air_quality_data/karachi_air_weather_2024.parquet"
df = pd.read_parquet(input_path)

# Ensure timestamp is datetime and sorted
df['timestamp'] = pd.to_datetime(df['timestamp'])
df = df.sort_values('timestamp').reset_index(drop=True)

# Rename to simpler column names we'll use below
df = df.rename(columns={
    'temp_C': 'temp',
    'humidity_percent': 'humidity',
    'pressure_hPa': 'pressure',
    'wind_speed_mps': 'wind_speed',
    'wind_deg': 'wind_deg',
    'pm2_5_ugm3': 'pm25',
    'pm10_ugm3': 'pm10',
    'co_ugm3': 'co',
    'no2_ugm3': 'no2',
    'so2_ugm3': 'so2',
    'o3_ugm3': 'o3'
})

# --- 1) Time-based features ---
df['hour'] = df['timestamp'].dt.hour
df['day_of_week'] = df['timestamp'].dt.dayofweek
df['month'] = df['timestamp'].dt.month
df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)

# --- 2) Safe missing-value handling for numeric columns used in featurization ---
# we will not drop rows; instead keep NaNs so feature store keeps consistent schema.
numeric_cols = ['pm25','pm10','co','no2','so2','o3','temp','humidity','pressure','wind_speed']
for c in numeric_cols:
    if c not in df.columns:
        df[c] = np.nan
# optional: forward-fill small gaps for rolling computations (won't overwrite long gaps)
df[numeric_cols] = df[numeric_cols].ffill(limit=3)

# --- 3) AQI-related derived features ---
# hour-to-hour change of pm2.5 (proxy for AQI change rate). Keep NaN where not computable.
df['pm25_diff_1h'] = df['pm25'].diff()

# rolling means and stds (min_periods=1 to compute at start)
df['pm25_rollmean_6h'] = df['pm25'].rolling(window=6, min_periods=1).mean()
df['pm25_rollmean_24h'] = df['pm25'].rolling(window=24, min_periods=1).mean()
df['pm25_rollstd_24h'] = df['pm25'].rolling(window=24, min_periods=1).std().fillna(0.0)

# proportion relative to 24h mean (use safe divide)
df['pm25_over_24h_mean'] = df['pm25'] / (df['pm25_rollmean_24h'].replace({0: np.nan}))

# --- 4) Weather-interaction features ---
df['temp_x_humidity'] = df['temp'] * df['humidity']
# wind inverse: calm conditions (low wind) often correlate with worse AQI
df['wind_inverse'] = 1.0 / (df['wind_speed'].fillna(0.0) + 0.1)

# convert wind direction (cyclic) to sin/cos for ML
if 'wind_deg' in df.columns:
    df['wind_sin'] = np.sin(np.deg2rad(df['wind_deg'].fillna(0.0)))
    df['wind_cos'] = np.cos(np.deg2rad(df['wind_deg'].fillna(0.0)))
else:
    df['wind_sin'] = np.nan
    df['wind_cos'] = np.nan

# --- 5) Lag features (common lags for short-term forecasting) ---
df['pm25_lag_1h'] = df['pm25'].shift(1)
df['pm25_lag_3h'] = df['pm25'].shift(3)
df['pm25_lag_24h'] = df['pm25'].shift(24)

# you can add lags for other pollutants similarly if desired

# --- 6) Optional: flag rows with too many missing pollutant measurements (for QA) ---
df['pollutant_null_count'] = df[['pm25','pm10','no2','o3','so2','co']].isna().sum(axis=1)
# create a boolean flag: True if majority of pollutants missing
df['pollutant_data_sparse'] = (df['pollutant_null_count'] >= 4)

# --- 7) Reorder columns for readability and save ---
cols_prefer = ['timestamp','city','latitude','longitude',
               'pm25','pm10','no2','o3','so2','co',
               'pm25_lag_1h','pm25_lag_3h','pm25_lag_24h',
               'pm25_diff_1h','pm25_rollmean_6h','pm25_rollmean_24h','pm25_rollstd_24h','pm25_over_24h_mean',
               'temp','humidity','pressure','wind_speed','wind_deg','wind_sin','wind_cos','wind_inverse',
               'temp_x_humidity','hour','day_of_week','month','is_weekend',
               'pollutant_data_sparse']

# only keep existing columns in the above order
existing_cols = [c for c in cols_prefer if c in df.columns]
df_features = df[existing_cols].copy()

# create output folder and save
output_path = "/content/drive/MyDrive/air_quality_data/"
os.makedirs(output_path, exist_ok=True)
out_file = os.path.join(output_path, "karachi_air_features_2024.parquet")
df_features.to_parquet(out_file, index=False)

print("Feature file saved to:", out_file)
print("Rows:", len(df_features))
df_features.head(20)

"""Starting with HOPSWORKS


    
"""

!pip install hopsworks
!pip install hopsworks==4.2
!pip install confluent-kafka

!pip uninstall -y hopsworks hsfs
!pip install hopsworks==4.4.2 hsfs==3.7.9 --quiet

#pip uninstall hopsworks hsfs

#pip install hopsworks

import hopsworks
import pandas as pd

#  Connect to your Hopsworks project
project = hopsworks.login(api_key_value="jUnWmY5gnbOgCTFa.mMZBk1yLzRJtocPY0Eq5alolUXu6sc1Ol2iFIO75SXZZaFCLdN4JY7i6ryPEV8yJ")
fs = project.get_feature_store()

#  Load your dataset (from Drive or local)
# Example: Adjust path if your file is in Drive
data = pd.read_csv("/content/drive/MyDrive/air_quality_data/karachi_air_weather_2024.csv")

#  Ensure timestamp column is datetime
data["timestamp"] = pd.to_datetime(data["timestamp"])

#  Create a Feature Group
feature_group = fs.get_or_create_feature_group(
    name="air_quality_features",
    version=1,
    primary_key=["city", "timestamp"],
    description="Weather and air-quality features for karachi (1-year historical data)"
)

# üöÄ Upload to Hopsworks Feature Store
feature_group.insert(data)

import hopsworks
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

#  Reconnect to your project
project = hopsworks.login(api_key_value="jUnWmY5gnbOgCTFa.mMZBk1yLzRJtocPY0Eq5alolUXu6sc1Ol2iFIO75SXZZaFCLdN4JY7i6ryPEV8yJ")
fs = project.get_feature_store()

# üì• Load feature data from Feature Group
feature_group = fs.get_feature_group(name="air_quality_features", version=1)
df = feature_group.read()  # Reads offline data

# üßπ Basic preprocessing
df = df.dropna()

# üéØ Target and features
target = "pm2_5_ugm3"
X = df.drop(columns=[target, "city", "timestamp"])
y = df[target]

# ‚úÇÔ∏è Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ü§ñ Train
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# üìä Evaluate
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# ‚ûï Add RMSE calculation here
from sklearn.metrics import mean_squared_error
import numpy as np
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print("\n" + "="*50)
print("           ‚ú® MODEL TRAINING COMPLETE ‚ú®")
print("="*50)
print(f"Algorithm: Random Forest Regressor")
print("-" * 50)
print(f"METRICS:")
print(f"| Mean Absolute Error (MAE): {mae:.2f}")
print(f"| Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"| R-Squared (R¬≤ Score):      {r2:.3f}")
print("-" * 50)


# üíæ Save model locally
joblib.dump(model, "pm25_rf_model.pkl")
print("Model saved as pm25_rf_model.pkl")

#EDA

import matplotlib.pyplot as plt
import seaborn as sns

# üìà Basic overview
print("Data shape:", df.shape)
print("\nData summary:")
display(df.describe())

# üåÜ PM2.5 distribution
plt.figure(figsize=(8,5))
sns.histplot(df['pm2_5_ugm3'], bins=30, kde=True, color='skyblue')
plt.title("Distribution of PM2.5 Levels")
plt.xlabel("PM2.5 (¬µg/m¬≥)")
plt.ylabel("Frequency")
plt.show()

# üïí PM2.5 trend over time (for one sample city)
sample_city = df['city'].unique()[0]
city_data = df[df['city'] == sample_city].sort_values('timestamp')

plt.figure(figsize=(10,5))
plt.plot(city_data['timestamp'], city_data['pm2_5_ugm3'], marker='', linestyle='-', color='teal')
plt.title(f"PM2.5 Trend Over Time ‚Äî {sample_city}")
plt.xlabel("Timestamp")
plt.ylabel("PM2.5 (¬µg/m¬≥)")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 1‚ø§ Register trained model in Hopsworks

import hopsworks
import joblib
import json
from google.colab import userdata

# üîë Reconnect to your project
try:
    project = hopsworks.login(api_key_value=userdata.get("HOPSWORKS_API_KEY"))
except Exception as e:
    print(f"Error connecting to Hopsworks: {e}")
    print("Please make sure your API key is stored in Colab secrets with the name 'HOPSWORKS_API_KEY'.")
    raise

mr = project.get_model_registry()

# üß† Load your trained model (saved previously)
model = joblib.load("pm25_rf_model.pkl")

# üìä Define metadata and metrics
# You can dynamically get metrics from the trained model evaluation if needed
# For now, using the hardcoded values from the previous successful run
metrics = {
    "mae": 2.33,
    "r2": 0.928
}

# üìù Create a model entry in Hopsworks Model Registry
model_registry_entry = mr.python.create_model(
    name="pm25_random_forest_model",
    metrics=metrics,
    description="Random Forest model predicting PM2.5 concentrations using weather and pollutant features for karachi (2024 data)",
    input_example=None,  # Optional: can provide df_sample.head(1).to_dict() if desired
)

# üöÄ Upload the model file
model_registry_entry.save("pm25_rf_model.pkl")

print("‚úÖ Model successfully registered in Hopsworks!")
# Print individual components
print("Hopsworks URL:", project.get_url())
print("Project ID:", project.id)
# Construct the correct URL by removing the duplicated path if present
base_url = project.get_url().split('/p/')[0]
print("Check models here:", f"{base_url}/p/{project.id}/models")
print("Check this specific model here:", f"{base_url}/p/{project.id}/models/{model_registry_entry.name}/{model_registry_entry.version}")

# üß† Train & save Ridge Regression model before registering
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Save the Ridge model
import joblib
joblib.dump(ridge, "pm25_ridge_model.pkl")

print("‚úÖ Ridge Regression model saved as pm25_ridge_model.pkl")

# Ridge regression
import hopsworks
import joblib
import datetime
from google.colab import userdata

# Connect to Hopsworks
try:
    project = hopsworks.login(api_key_value=userdata.get("HOPSWORKS_API_KEY"))
except Exception as e:
    print(f"Error connecting to Hopsworks: {e}")
    print("Please make sure your API key is stored in Colab secrets with the name 'HOPSWORKS_API_KEY'.")
    raise

mr = project.get_model_registry()

# Register Random Forest model
rf_model = joblib.load("pm25_rf_model.pkl")
rf_model_meta = mr.python.create_model(
    name="pm25_random_forest_model",
    metrics={"mae": 4.99, "r2": 0.928},
    description="Random Forest model predicting PM2.5 levels using weather and pollutant features."
)
rf_model_meta.save("pm25_rf_model.pkl")


# Register Ridge Regression model
ridge_model = joblib.load("pm25_ridge_model.pkl")
ridge_model_meta = mr.python.create_model(
    name="pm25_ridge_model",
    metrics={"mae": 11.37, "r2": 0.849},
    description="Ridge Regression model used as a linear baseline for PM2.5 forecasting."
)
ridge_model_meta.save("pm25_ridge_model.pkl")


print("‚úÖ Both models registered¬†successfully!")

# AQI CALCULATOR & PREDICTOR

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# 1Ô∏è AQI CALCULATION FUNCTION (US EPA Standard)

def calculate_aqi(pm25, pm10):
    """
    Calculate AQI based on PM2.5 and PM10 using US EPA breakpoints.
    Returns the higher AQI value between PM2.5 and PM10.
    """

    def get_aqi_pm25(pm25):
        if pd.isna(pm25):
            return np.nan

        # PM2.5 breakpoints (¬µg/m¬≥) and corresponding AQI values
        breakpoints = [
            (0.0, 12.0, 0, 50),
            (12.1, 35.4, 51, 100),
            (35.5, 55.4, 101, 150),
            (55.5, 150.4, 151, 200),
            (150.5, 250.4, 201, 300),
            (250.5, 500.4, 301, 500)
        ]

        for bp_lo, bp_hi, aqi_lo, aqi_hi in breakpoints:
            if bp_lo <= pm25 <= bp_hi:
                aqi = ((aqi_hi - aqi_lo) / (bp_hi - bp_lo)) * (pm25 - bp_lo) + aqi_lo
                return round(aqi)

        return 500  # Above 500.4

    def get_aqi_pm10(pm10):
        if pd.isna(pm10):
            return np.nan

        # PM10 breakpoints (¬µg/m¬≥) and corresponding AQI values
        breakpoints = [
            (0, 54, 0, 50),
            (55, 154, 51, 100),
            (155, 254, 101, 150),
            (255, 354, 151, 200),
            (355, 424, 201, 300),
            (425, 604, 301, 500)
        ]

        for bp_lo, bp_hi, aqi_lo, aqi_hi in breakpoints:
            if bp_lo <= pm10 <= bp_hi:
                aqi = ((aqi_hi - aqi_lo) / (bp_hi - bp_lo)) * (pm10 - bp_lo) + aqi_lo
                return round(aqi)

        return 500

    aqi_pm25 = get_aqi_pm25(pm25)
    aqi_pm10 = get_aqi_pm10(pm10)

    # Return the maximum AQI (worse air quality)
    if pd.isna(aqi_pm25) and pd.isna(aqi_pm10):
        return np.nan
    elif pd.isna(aqi_pm25):
        return aqi_pm10
    elif pd.isna(aqi_pm10):
        return aqi_pm25
    else:
        return max(aqi_pm25, aqi_pm10)


def get_aqi_category(aqi):
    """Return AQI category and color"""
    if pd.isna(aqi):
        return "Unknown", "#808080"
    elif aqi <= 50:
        return "Good", "#00E400"
    elif aqi <= 100:
        return "Moderate", "#FFFF00"
    elif aqi <= 150:
        return "Unhealthy for Sensitive Groups", "#FF7E00"
    elif aqi <= 200:
        return "Unhealthy", "#FF0000"
    elif aqi <= 300:
        return "Very Unhealthy", "#8F3F97"
    else:
        return "Hazardous", "#7E0023"

# 2Ô∏è ADD AQI TO YOUR FEATURE DATASET

# Load your feature dataset
input_path = "/content/drive/MyDrive/air_quality_data/karachi_air_features_2024.parquet"
df = pd.read_parquet(input_path)

# Calculate AQI for each row
print("Calculating AQI values...")
df['aqi'] = df.apply(lambda row: calculate_aqi(row['pm25'], row['pm10']), axis=1)

# Add AQI category
df['aqi_category'] = df['aqi'].apply(lambda x: get_aqi_category(x)[0])

print(f"‚úÖ AQI calculated for {len(df)} rows")
print(f"Average AQI: {df['aqi'].mean():.1f}")
print(f"\nAQI Distribution:")
print(df['aqi_category'].value_counts())

# Save updated dataset with AQI (both CSV and Parquet)
output_path = "/content/drive/MyDrive/air_quality_data/"

# Save as Parquet
df.to_parquet(output_path + "karachi_air_features_with_aqi_2024.parquet", index=False)
print(f"\nüíæ Saved Parquet to: {output_path}karachi_air_features_with_aqi_2024.parquet")

# Save as CSV
df.to_csv(output_path + "karachi_air_features_with_aqi_2024.csv", index=False)
print(f"üíæ Saved CSV to: {output_path}karachi_air_features_with_aqi_2024.csv")

# Also save a simplified version with just the main columns
df_simple = df[['timestamp', 'city', 'latitude', 'longitude',
                'pm25', 'pm10', 'no2', 'o3', 'so2', 'co',
                'temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg',
                'aqi', 'aqi_category']].copy()

df_simple.to_csv(output_path + "karachi_air_with_aqi_simple_2024.csv", index=False)
print(f"üíæ Saved simplified CSV to: {output_path}karachi_air_with_aqi_simple_2024.csv")

print(f"\nüìä Summary:")
print(f"   Total rows: {len(df)}")
print(f"   Columns in full dataset: {len(df.columns)}")
print(f"   Columns in simple dataset: {len(df_simple.columns)}")

# 3Ô∏è‚É£ TRAIN AQI PREDICTION MODEL

print("\n" + "="*60)
print("           ü§ñ TRAINING AQI PREDICTION MODEL")
print("="*60)

# Prepare data for modeling
df_model = df.dropna(subset=['aqi'])  # Remove rows without AQI

# Select features for prediction
feature_cols = [
    'pm25', 'pm10', 'no2', 'o3', 'so2', 'co',
    'pm25_lag_1h', 'pm25_lag_3h', 'pm25_lag_24h',
    'pm25_diff_1h', 'pm25_rollmean_6h', 'pm25_rollmean_24h',
    'pm25_rollstd_24h', 'pm25_over_24h_mean',
    'temp', 'humidity', 'pressure', 'wind_speed',
    'wind_sin', 'wind_cos', 'wind_inverse',
    'temp_x_humidity', 'hour', 'day_of_week', 'month', 'is_weekend'
]

# Filter only existing columns
feature_cols = [col for col in feature_cols if col in df_model.columns]

X = df_model[feature_cols].fillna(0)  # Fill NaN with 0 for model
y = df_model['aqi']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\nüìä Dataset split:")
print(f"   Training samples: {len(X_train)}")
print(f"   Testing samples:  {len(X_test)}")

# Train Multiple Models

models = {}

# 1. Random Forest
print("\nüå≤ Training Random Forest...")
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=20,
    min_samples_split=5,
    random_state=42,
    n_jobs=-1
)
rf_model.fit(X_train, y_train)
models['random_forest'] = rf_model

# 2. Gradient Boosting
print("üöÄ Training Gradient Boosting...")
gb_model = GradientBoostingRegressor(
    n_estimators=200,
    max_depth=7,
    learning_rate=0.1,
    random_state=42
)
gb_model.fit(X_train, y_train)
models['gradient_boosting'] = gb_model

# 4Ô∏è EVALUATE MODELS

print("\n" + "="*60)
print("           üìä MODEL PERFORMANCE COMPARISON")
print("="*60)

results = []

for name, model in models.items():
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    results.append({
        'Model': name.replace('_', ' ').title(),
        'MAE': mae,
        'RMSE': rmse,
        'R¬≤': r2
    })

    print(f"\n{name.replace('_', ' ').upper()}:")
    print(f"  MAE:  {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  R¬≤:   {r2:.4f}")

results_df = pd.DataFrame(results)
print("\n" + "="*60)
print(results_df.to_string(index=False))
print("="*60)

# 5Ô∏è SAVE BEST MODEL (Random Forest typically performs best)

best_model = rf_model  # or choose based on metrics
joblib.dump(best_model, "aqi_prediction_model.pkl")
print("\n‚úÖ Best model saved as: aqi_prediction_model.pkl")

# Save feature list for future predictions
with open("aqi_model_features.txt", "w") as f:
    f.write("\n".join(feature_cols))
print("‚úÖ Feature list saved as: aqi_model_features.txt")

# 6Ô∏è‚É£ FEATURE IMPORTANCE ANALYSIS

print("\n" + "="*60)
print("           üîç TOP 10 MOST IMPORTANT FEATURES")
print("="*60)

feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': best_model.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance.head(10).to_string(index=False))

# 7Ô∏è REGISTER AQI MODEL IN HOPSWORKS

print("\n" + "="*60)
print("           üì§ REGISTERING MODEL IN HOPSWORKS")
print("="*60)

import hopsworks
from google.colab import userdata

try:
    project = hopsworks.login(api_key_value=userdata.get("HOPSWORKS_API_KEY"))
    mr = project.get_model_registry()

    # Get best model metrics
    best_metrics = results_df[results_df['Model'] == 'Random Forest'].iloc[0].to_dict()

    aqi_model_meta = mr.python.create_model(
        name="aqi_prediction_model",
        metrics={
            "mae": float(best_metrics['MAE']),
            "rmse": float(best_metrics['RMSE']),
            "r2": float(best_metrics['R¬≤'])
        },
        description="AQI prediction model using Random Forest with weather and pollutant features for Karachi (2024 data)"
    )

    aqi_model_meta.save("aqi_prediction_model.pkl")

    print("‚úÖ AQI model successfully registered in Hopsworks!")
    base_url = project.get_url().split('/p/')[0]
    print(f"View model: {base_url}/p/{project.id}/models/{aqi_model_meta.name}/{aqi_model_meta.version}")

except Exception as e:
    print(f"‚ö†Ô∏è  Could not register in Hopsworks: {e}")
    print("Model is still saved locally as aqi_prediction_model.pkl")

# 8Ô∏è UPDATE FEATURE STORE WITH AQI

print("\n" + "="*60)
print("           üì§ UPDATING FEATURE STORE WITH AQI")
print("="*60)

try:
    fs = project.get_feature_store()

    # Prepare data for feature store
    df_upload = df[['timestamp', 'city', 'latitude', 'longitude',
                     'pm25', 'pm10', 'no2', 'o3', 'so2', 'co',
                     'temp', 'humidity', 'pressure', 'wind_speed', 'wind_deg',
                     'aqi', 'aqi_category']].copy()

    # Create or get feature group with AQI
    aqi_feature_group = fs.get_or_create_feature_group(
        name="air_quality_with_aqi",
        version=1,
        primary_key=["city", "timestamp"],
        description="Weather, air-quality features, and calculated AQI for Karachi (2024 data)"
    )

    aqi_feature_group.insert(df_upload)
    print("‚úÖ Feature store updated with AQI data!")

except Exception as e:
    print(f"‚ö†Ô∏è  Could not update feature store: {e}")

# 9Ô∏è QUICK PREDICTION EXAMPLE

print("\n" + "="*60)
print("           üîÆ SAMPLE PREDICTION")
print("="*60)

# Take a sample from test set
sample = X_test.iloc[0:1]
predicted_aqi = best_model.predict(sample)[0]
actual_aqi = y_test.iloc[0]

category, color = get_aqi_category(predicted_aqi)

print(f"\nPredicted AQI: {predicted_aqi:.1f} ({category})")
print(f"Actual AQI:    {actual_aqi:.1f}")
print(f"Difference:    {abs(predicted_aqi - actual_aqi):.1f}")

print("\n" + "="*60)
print("           ‚ú® AQI PREDICTOR SETUP COMPLETE! ‚ú®")
print("="*60)